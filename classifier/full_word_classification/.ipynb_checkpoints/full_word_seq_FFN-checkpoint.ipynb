{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, Flatten, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.utils import np_utils\n",
    "#sklearn imports\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362237\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x12d2ef898>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_pickle('../lyrics.pkl', compression='gzip')\n",
    "print(len(df))\n",
    "sns.countplot(df.genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>song</th>\n",
       "      <th>year</th>\n",
       "      <th>artist</th>\n",
       "      <th>genre</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>genre</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Hip-Hop</th>\n",
       "      <td>24850</td>\n",
       "      <td>21479</td>\n",
       "      <td>32</td>\n",
       "      <td>1107</td>\n",
       "      <td>1</td>\n",
       "      <td>22648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metal</th>\n",
       "      <td>23759</td>\n",
       "      <td>21852</td>\n",
       "      <td>33</td>\n",
       "      <td>996</td>\n",
       "      <td>1</td>\n",
       "      <td>22498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pop</th>\n",
       "      <td>40466</td>\n",
       "      <td>32443</td>\n",
       "      <td>47</td>\n",
       "      <td>2066</td>\n",
       "      <td>1</td>\n",
       "      <td>37229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rock</th>\n",
       "      <td>109235</td>\n",
       "      <td>85240</td>\n",
       "      <td>48</td>\n",
       "      <td>3765</td>\n",
       "      <td>1</td>\n",
       "      <td>102512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          index   song  year  artist  genre  lyrics\n",
       "genre                                              \n",
       "Hip-Hop   24850  21479    32    1107      1   22648\n",
       "Metal     23759  21852    33     996      1   22498\n",
       "Pop       40466  32443    47    2066      1   37229\n",
       "Rock     109235  85240    48    3765      1  102512"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#probably a better way to do this\n",
    "df = df[(df.genre == 'Pop') | (df.genre == 'Hip-Hop') | (df.genre == 'Metal') | (df.genre == 'Rock')]\n",
    "df.dropna(inplace=True)\n",
    "df.groupby('genre').nunique('genre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x12c690358>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#also probably a better way to do this using 28408 because that is the smallest count\n",
    "pop = df.loc[df['genre']=='Pop'].head(1000)\n",
    "hiphop = df.loc[df['genre']=='Hip-Hop'].head(1000)\n",
    "rock = df.loc[df['genre']=='Rock'].head(1000)\n",
    "metal = df.loc[df['genre']=='Metal'].head(1000)\n",
    "#really ratchet\n",
    "output = pop.append(hiphop)\n",
    "output = output.append(rock)\n",
    "output = output.append(metal)\n",
    "output.dropna(inplace=True)\n",
    "output.reset_index(drop=True, inplace=True)\n",
    "print(len(output))\n",
    "#but it works...\n",
    "sns.countplot(output.genre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess data\n",
    "-  make our 10 word sequences integer encoded\n",
    "- use one-hot encoding to encode our genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating our feature set w/Keras tokenizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b73b78550237>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mMAX_VALUE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;31m#the length of each of our sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_VALUE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlyrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#build sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlyrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/py36/lib/python3.6/site-packages/keras_preprocessing/text.py\u001b[0m in \u001b[0;36mfit_on_texts\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                 \u001b[0;31m# In how many documents each word occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mwcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#this value is the length of the longest song in our dataset.\n",
    "\n",
    "tok = Tokenizer(num_words=MAX_VALUE)\n",
    "tok.fit_on_texts(df.lyrics)\n",
    "#build sequences\n",
    "sequences = tok.texts_to_sequences(output.lyrics)\n",
    "sequences_matrix = sequence.pad_sequences(sequences, maxlen=MAX_VALUE)\n",
    "sequences_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating our target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "integer_encoded = label_encoder.fit_transform(output.genre)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split into test/train 80/20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(sequences_matrix,onehot_encoded,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=X_train[0].shape, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import TensorBoard\n",
    "tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0,\n",
    "                          write_graph=True, write_images=False)\n",
    "optimizer = RMSprop(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30589 samples, validate on 7648 samples\n",
      "Epoch 1/100\n",
      "30589/30589 [==============================] - 0s 13us/step - loss: 1.3882 - acc: 0.2731 - val_loss: 1.3820 - val_acc: 0.2891\n",
      "Epoch 2/100\n",
      "30589/30589 [==============================] - 0s 9us/step - loss: 1.3792 - acc: 0.2907 - val_loss: 1.3769 - val_acc: 0.2926\n",
      "Epoch 3/100\n",
      "30589/30589 [==============================] - 0s 9us/step - loss: 1.3757 - acc: 0.2913 - val_loss: 1.3760 - val_acc: 0.2862\n",
      "Epoch 4/100\n",
      "30589/30589 [==============================] - 0s 9us/step - loss: 1.3725 - acc: 0.2968 - val_loss: 1.3732 - val_acc: 0.2924\n",
      "Epoch 5/100\n",
      "30589/30589 [==============================] - 0s 9us/step - loss: 1.3706 - acc: 0.2958 - val_loss: 1.3683 - val_acc: 0.3001\n",
      "Epoch 6/100\n",
      "30589/30589 [==============================] - 0s 9us/step - loss: 1.3685 - acc: 0.2988 - val_loss: 1.3662 - val_acc: 0.2984\n",
      "Epoch 7/100\n",
      "30589/30589 [==============================] - 0s 10us/step - loss: 1.3668 - acc: 0.3006 - val_loss: 1.3639 - val_acc: 0.3037\n",
      "Epoch 8/100\n",
      "30589/30589 [==============================] - 0s 10us/step - loss: 1.3658 - acc: 0.3047 - val_loss: 1.3712 - val_acc: 0.3001\n",
      "Epoch 9/100\n",
      "30589/30589 [==============================] - 0s 9us/step - loss: 1.3645 - acc: 0.3033 - val_loss: 1.3654 - val_acc: 0.3033\n",
      "Epoch 10/100\n",
      "30589/30589 [==============================] - 0s 9us/step - loss: 1.3633 - acc: 0.3068 - val_loss: 1.3628 - val_acc: 0.3019\n",
      "Epoch 11/100\n",
      "30589/30589 [==============================] - 0s 10us/step - loss: 1.3616 - acc: 0.3098 - val_loss: 1.3613 - val_acc: 0.3022\n",
      "Epoch 12/100\n",
      "30589/30589 [==============================] - 0s 9us/step - loss: 1.3616 - acc: 0.3105 - val_loss: 1.3604 - val_acc: 0.3145\n",
      "Epoch 13/100\n",
      "30589/30589 [==============================] - 0s 11us/step - loss: 1.3606 - acc: 0.3114 - val_loss: 1.3640 - val_acc: 0.3142\n",
      "Epoch 14/100\n",
      "30589/30589 [==============================] - 0s 11us/step - loss: 1.3597 - acc: 0.3129 - val_loss: 1.3624 - val_acc: 0.3146\n",
      "Epoch 15/100\n",
      "30589/30589 [==============================] - 0s 9us/step - loss: 1.3591 - acc: 0.3119 - val_loss: 1.3609 - val_acc: 0.3236\n",
      "Epoch 16/100\n",
      "30589/30589 [==============================] - 0s 16us/step - loss: 1.3582 - acc: 0.3176 - val_loss: 1.3621 - val_acc: 0.3230\n",
      "Epoch 17/100\n",
      "30589/30589 [==============================] - 0s 14us/step - loss: 1.3583 - acc: 0.3151 - val_loss: 1.3604 - val_acc: 0.3150\n",
      "Epoch 18/100\n",
      "30589/30589 [==============================] - 0s 9us/step - loss: 1.3584 - acc: 0.3149 - val_loss: 1.3576 - val_acc: 0.3251\n",
      "Epoch 19/100\n",
      "30589/30589 [==============================] - 0s 11us/step - loss: 1.3569 - acc: 0.3183 - val_loss: 1.3570 - val_acc: 0.3260\n",
      "Epoch 20/100\n",
      "30589/30589 [==============================] - 0s 13us/step - loss: 1.3572 - acc: 0.3149 - val_loss: 1.3570 - val_acc: 0.3223\n",
      "Epoch 21/100\n",
      "30589/30589 [==============================] - 0s 11us/step - loss: 1.3566 - acc: 0.3188 - val_loss: 1.3556 - val_acc: 0.3283\n",
      "Epoch 22/100\n",
      "30589/30589 [==============================] - 0s 10us/step - loss: 1.3561 - acc: 0.3200 - val_loss: 1.3600 - val_acc: 0.3224\n",
      "Epoch 23/100\n",
      "30589/30589 [==============================] - 0s 9us/step - loss: 1.3559 - acc: 0.3207 - val_loss: 1.3574 - val_acc: 0.3185\n",
      "Epoch 24/100\n",
      "30589/30589 [==============================] - 0s 10us/step - loss: 1.3556 - acc: 0.3179 - val_loss: 1.3558 - val_acc: 0.3264\n",
      "Epoch 25/100\n",
      "30589/30589 [==============================] - 0s 9us/step - loss: 1.3550 - acc: 0.3186 - val_loss: 1.3586 - val_acc: 0.3135\n",
      "Epoch 26/100\n",
      "30589/30589 [==============================] - 0s 9us/step - loss: 1.3548 - acc: 0.3221 - val_loss: 1.3581 - val_acc: 0.3256\n",
      "Epoch 27/100\n",
      "30589/30589 [==============================] - 0s 9us/step - loss: 1.3544 - acc: 0.3203 - val_loss: 1.3574 - val_acc: 0.3166\n",
      "Epoch 28/100\n",
      "30589/30589 [==============================] - 0s 10us/step - loss: 1.3537 - acc: 0.3211 - val_loss: 1.3578 - val_acc: 0.3248\n",
      "Epoch 29/100\n",
      "30589/30589 [==============================] - 0s 9us/step - loss: 1.3537 - acc: 0.3190 - val_loss: 1.3545 - val_acc: 0.3197\n",
      "Epoch 30/100\n",
      "30589/30589 [==============================] - 0s 12us/step - loss: 1.3537 - acc: 0.3207 - val_loss: 1.3602 - val_acc: 0.3125\n",
      "Epoch 31/100\n",
      "30589/30589 [==============================] - 0s 10us/step - loss: 1.3530 - acc: 0.3201 - val_loss: 1.3595 - val_acc: 0.3132\n",
      "Epoch 32/100\n",
      "30589/30589 [==============================] - 0s 9us/step - loss: 1.3523 - acc: 0.3222 - val_loss: 1.3582 - val_acc: 0.3248\n",
      "Epoch 33/100\n",
      "30589/30589 [==============================] - 0s 11us/step - loss: 1.3525 - acc: 0.3222 - val_loss: 1.3554 - val_acc: 0.3296\n",
      "Epoch 34/100\n",
      "30589/30589 [==============================] - 0s 10us/step - loss: 1.3513 - acc: 0.3243 - val_loss: 1.3585 - val_acc: 0.3223\n",
      "Epoch 35/100\n",
      "30589/30589 [==============================] - 0s 9us/step - loss: 1.3515 - acc: 0.3214 - val_loss: 1.3599 - val_acc: 0.3232\n",
      "Epoch 36/100\n",
      "30589/30589 [==============================] - 0s 9us/step - loss: 1.3517 - acc: 0.3206 - val_loss: 1.3564 - val_acc: 0.3201\n",
      "Epoch 37/100\n",
      "30589/30589 [==============================] - 0s 9us/step - loss: 1.3509 - acc: 0.3234 - val_loss: 1.3582 - val_acc: 0.3164\n",
      "Epoch 38/100\n",
      "30589/30589 [==============================] - 0s 11us/step - loss: 1.3511 - acc: 0.3228 - val_loss: 1.3543 - val_acc: 0.3228\n",
      "Epoch 39/100\n",
      "30589/30589 [==============================] - 0s 9us/step - loss: 1.3507 - acc: 0.3234 - val_loss: 1.3568 - val_acc: 0.3231\n",
      "Epoch 40/100\n",
      "30589/30589 [==============================] - 0s 10us/step - loss: 1.3503 - acc: 0.3262 - val_loss: 1.3562 - val_acc: 0.3269\n",
      "Epoch 41/100\n",
      "30589/30589 [==============================] - 0s 10us/step - loss: 1.3500 - acc: 0.3243 - val_loss: 1.3532 - val_acc: 0.3322\n",
      "Epoch 42/100\n",
      "30589/30589 [==============================] - 0s 9us/step - loss: 1.3493 - acc: 0.3243 - val_loss: 1.3559 - val_acc: 0.3245\n",
      "Epoch 43/100\n",
      "30589/30589 [==============================] - 0s 11us/step - loss: 1.3496 - acc: 0.3226 - val_loss: 1.3539 - val_acc: 0.3304\n",
      "Epoch 44/100\n",
      "30589/30589 [==============================] - 0s 9us/step - loss: 1.3495 - acc: 0.3259 - val_loss: 1.3550 - val_acc: 0.3247\n",
      "Epoch 45/100\n",
      "30589/30589 [==============================] - 0s 9us/step - loss: 1.3492 - acc: 0.3272 - val_loss: 1.3614 - val_acc: 0.3264\n",
      "Epoch 46/100\n",
      "30589/30589 [==============================] - 0s 10us/step - loss: 1.3484 - acc: 0.3312 - val_loss: 1.3550 - val_acc: 0.3317\n",
      "Epoch 47/100\n",
      "30589/30589 [==============================] - 0s 13us/step - loss: 1.3484 - acc: 0.3254 - val_loss: 1.3558 - val_acc: 0.3245\n",
      "Epoch 48/100\n",
      "30589/30589 [==============================] - 0s 10us/step - loss: 1.3483 - acc: 0.3261 - val_loss: 1.3543 - val_acc: 0.3200\n",
      "Epoch 49/100\n",
      "30589/30589 [==============================] - 0s 10us/step - loss: 1.3474 - acc: 0.3292 - val_loss: 1.3555 - val_acc: 0.3341\n",
      "Epoch 50/100\n",
      "30589/30589 [==============================] - 0s 12us/step - loss: 1.3477 - acc: 0.3288 - val_loss: 1.3551 - val_acc: 0.3315\n",
      "Epoch 51/100\n",
      "30589/30589 [==============================] - 0s 11us/step - loss: 1.3475 - acc: 0.3286 - val_loss: 1.3582 - val_acc: 0.3184\n",
      "Epoch 52/100\n",
      "30589/30589 [==============================] - 0s 11us/step - loss: 1.3473 - acc: 0.3275 - val_loss: 1.3570 - val_acc: 0.3210\n",
      "Epoch 53/100\n",
      "30589/30589 [==============================] - 0s 11us/step - loss: 1.3464 - acc: 0.3313 - val_loss: 1.3531 - val_acc: 0.3308\n",
      "Epoch 54/100\n",
      "30589/30589 [==============================] - 0s 9us/step - loss: 1.3471 - acc: 0.3296 - val_loss: 1.3542 - val_acc: 0.3303\n",
      "Epoch 55/100\n",
      "30589/30589 [==============================] - 0s 10us/step - loss: 1.3457 - acc: 0.3302 - val_loss: 1.3558 - val_acc: 0.3258\n",
      "Epoch 56/100\n",
      "30589/30589 [==============================] - 0s 10us/step - loss: 1.3459 - acc: 0.3287 - val_loss: 1.3609 - val_acc: 0.3282\n",
      "Epoch 57/100\n",
      "30589/30589 [==============================] - 0s 11us/step - loss: 1.3463 - acc: 0.3288 - val_loss: 1.3533 - val_acc: 0.3291\n",
      "Epoch 58/100\n",
      "30589/30589 [==============================] - 0s 10us/step - loss: 1.3455 - acc: 0.3299 - val_loss: 1.3546 - val_acc: 0.3322\n",
      "Epoch 59/100\n",
      "30589/30589 [==============================] - 0s 11us/step - loss: 1.3456 - acc: 0.3313 - val_loss: 1.3548 - val_acc: 0.3162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100\n",
      "30589/30589 [==============================] - 0s 12us/step - loss: 1.3451 - acc: 0.3284 - val_loss: 1.3569 - val_acc: 0.3333\n",
      "Epoch 61/100\n",
      "30589/30589 [==============================] - 0s 9us/step - loss: 1.3451 - acc: 0.3287 - val_loss: 1.3549 - val_acc: 0.3228\n",
      "Epoch 62/100\n",
      "30589/30589 [==============================] - 0s 9us/step - loss: 1.3449 - acc: 0.3341 - val_loss: 1.3592 - val_acc: 0.3141\n",
      "Epoch 63/100\n",
      "30589/30589 [==============================] - 0s 10us/step - loss: 1.3447 - acc: 0.3305 - val_loss: 1.3563 - val_acc: 0.3311\n",
      "Epoch 64/100\n",
      "30589/30589 [==============================] - 0s 13us/step - loss: 1.3442 - acc: 0.3332 - val_loss: 1.3611 - val_acc: 0.3313\n",
      "Epoch 65/100\n",
      "30589/30589 [==============================] - 0s 10us/step - loss: 1.3445 - acc: 0.3308 - val_loss: 1.3579 - val_acc: 0.3260\n",
      "Epoch 66/100\n",
      "30589/30589 [==============================] - 0s 10us/step - loss: 1.3436 - acc: 0.3327 - val_loss: 1.3570 - val_acc: 0.3273\n",
      "Epoch 67/100\n",
      "30589/30589 [==============================] - 0s 14us/step - loss: 1.3439 - acc: 0.3309 - val_loss: 1.3558 - val_acc: 0.3254\n",
      "Epoch 68/100\n",
      "30589/30589 [==============================] - 0s 10us/step - loss: 1.3437 - acc: 0.3320 - val_loss: 1.3572 - val_acc: 0.3298\n",
      "Epoch 69/100\n",
      "30589/30589 [==============================] - 0s 11us/step - loss: 1.3432 - acc: 0.3319 - val_loss: 1.3595 - val_acc: 0.3258\n",
      "Epoch 70/100\n",
      "30589/30589 [==============================] - 0s 12us/step - loss: 1.3430 - acc: 0.3319 - val_loss: 1.3588 - val_acc: 0.3248\n",
      "Epoch 71/100\n",
      "30589/30589 [==============================] - 0s 11us/step - loss: 1.3429 - acc: 0.3315 - val_loss: 1.3597 - val_acc: 0.3291\n",
      "Epoch 72/100\n",
      "30589/30589 [==============================] - 0s 12us/step - loss: 1.3429 - acc: 0.3322 - val_loss: 1.3555 - val_acc: 0.3320\n",
      "Epoch 73/100\n",
      "30589/30589 [==============================] - 0s 9us/step - loss: 1.3427 - acc: 0.3311 - val_loss: 1.3570 - val_acc: 0.3305\n",
      "Epoch 74/100\n",
      "30589/30589 [==============================] - 0s 11us/step - loss: 1.3423 - acc: 0.3354 - val_loss: 1.3566 - val_acc: 0.3300\n",
      "Epoch 75/100\n",
      "30589/30589 [==============================] - 0s 12us/step - loss: 1.3422 - acc: 0.3362 - val_loss: 1.3597 - val_acc: 0.3251\n",
      "Epoch 76/100\n",
      "30589/30589 [==============================] - 0s 11us/step - loss: 1.3418 - acc: 0.3343 - val_loss: 1.3583 - val_acc: 0.3190\n",
      "Epoch 77/100\n",
      "30589/30589 [==============================] - 0s 12us/step - loss: 1.3418 - acc: 0.3370 - val_loss: 1.3611 - val_acc: 0.3219\n",
      "Epoch 78/100\n",
      "30589/30589 [==============================] - 0s 12us/step - loss: 1.3418 - acc: 0.3354 - val_loss: 1.3571 - val_acc: 0.3222\n",
      "Epoch 79/100\n",
      "30589/30589 [==============================] - 0s 12us/step - loss: 1.3415 - acc: 0.3350 - val_loss: 1.3610 - val_acc: 0.3317\n",
      "Epoch 80/100\n",
      "30589/30589 [==============================] - 0s 10us/step - loss: 1.3416 - acc: 0.3380 - val_loss: 1.3574 - val_acc: 0.3285\n",
      "Epoch 81/100\n",
      "30589/30589 [==============================] - 0s 10us/step - loss: 1.3414 - acc: 0.3360 - val_loss: 1.3570 - val_acc: 0.3269\n",
      "Epoch 82/100\n",
      "30589/30589 [==============================] - 0s 10us/step - loss: 1.3414 - acc: 0.3355 - val_loss: 1.3558 - val_acc: 0.3358\n",
      "Epoch 83/100\n",
      "30589/30589 [==============================] - 0s 11us/step - loss: 1.3406 - acc: 0.3370 - val_loss: 1.3602 - val_acc: 0.3279\n",
      "Epoch 84/100\n",
      "30589/30589 [==============================] - 0s 9us/step - loss: 1.3402 - acc: 0.3374 - val_loss: 1.3599 - val_acc: 0.3279\n",
      "Epoch 85/100\n",
      "30589/30589 [==============================] - 0s 11us/step - loss: 1.3408 - acc: 0.3386 - val_loss: 1.3574 - val_acc: 0.3368\n",
      "Epoch 86/100\n",
      "30589/30589 [==============================] - 0s 12us/step - loss: 1.3404 - acc: 0.3370 - val_loss: 1.3569 - val_acc: 0.3326\n",
      "Epoch 87/100\n",
      "30589/30589 [==============================] - 0s 11us/step - loss: 1.3405 - acc: 0.3373 - val_loss: 1.3606 - val_acc: 0.3370\n",
      "Epoch 88/100\n",
      "30589/30589 [==============================] - 0s 10us/step - loss: 1.3397 - acc: 0.3371 - val_loss: 1.3645 - val_acc: 0.3283\n",
      "Epoch 89/100\n",
      "30589/30589 [==============================] - 0s 10us/step - loss: 1.3402 - acc: 0.3364 - val_loss: 1.3607 - val_acc: 0.3332\n",
      "Epoch 90/100\n",
      "30589/30589 [==============================] - 0s 10us/step - loss: 1.3402 - acc: 0.3355 - val_loss: 1.3577 - val_acc: 0.3319\n",
      "Epoch 91/100\n",
      "30589/30589 [==============================] - 1s 18us/step - loss: 1.3398 - acc: 0.3398 - val_loss: 1.3619 - val_acc: 0.3249\n",
      "Epoch 92/100\n",
      "30589/30589 [==============================] - 0s 10us/step - loss: 1.3398 - acc: 0.3380 - val_loss: 1.3587 - val_acc: 0.3313\n",
      "Epoch 93/100\n",
      "30589/30589 [==============================] - 0s 10us/step - loss: 1.3387 - acc: 0.3388 - val_loss: 1.3584 - val_acc: 0.3243\n",
      "Epoch 94/100\n",
      "30589/30589 [==============================] - 0s 11us/step - loss: 1.3396 - acc: 0.3404 - val_loss: 1.3573 - val_acc: 0.3328\n",
      "Epoch 95/100\n",
      "30589/30589 [==============================] - 0s 9us/step - loss: 1.3390 - acc: 0.3417 - val_loss: 1.3591 - val_acc: 0.3337\n",
      "Epoch 96/100\n",
      "30589/30589 [==============================] - 0s 9us/step - loss: 1.3389 - acc: 0.3369 - val_loss: 1.3614 - val_acc: 0.3300\n",
      "Epoch 97/100\n",
      "30589/30589 [==============================] - 0s 10us/step - loss: 1.3389 - acc: 0.3396 - val_loss: 1.3601 - val_acc: 0.3338\n",
      "Epoch 98/100\n",
      "30589/30589 [==============================] - 0s 9us/step - loss: 1.3384 - acc: 0.3390 - val_loss: 1.3590 - val_acc: 0.3282\n",
      "Epoch 99/100\n",
      "30589/30589 [==============================] - 0s 10us/step - loss: 1.3384 - acc: 0.3417 - val_loss: 1.3592 - val_acc: 0.3338\n",
      "Epoch 100/100\n",
      "30589/30589 [==============================] - 0s 9us/step - loss: 1.3385 - acc: 0.3401 - val_loss: 1.3593 - val_acc: 0.3247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x129ce07f0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=100, batch_size=128, validation_data=(X_test, Y_test), shuffle=True, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7648/7648 [==============================] - 0s 29us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3581676882181208, 0.31982217573221755]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=X_test, y=Y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=1000, output_dim=50, input_length=10))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, input_shape=X_train[0].shape, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30589 samples, validate on 7648 samples\n",
      "Epoch 1/100\n",
      "30589/30589 [==============================] - 1s 24us/step - loss: 1.3595 - acc: 0.3101 - val_loss: 1.3461 - val_acc: 0.3334\n",
      "Epoch 2/100\n",
      "30589/30589 [==============================] - 1s 17us/step - loss: 1.3509 - acc: 0.3262 - val_loss: 1.3441 - val_acc: 0.3313\n",
      "Epoch 3/100\n",
      "30589/30589 [==============================] - 1s 17us/step - loss: 1.3477 - acc: 0.3328 - val_loss: 1.3403 - val_acc: 0.3398\n",
      "Epoch 4/100\n",
      "30589/30589 [==============================] - 1s 17us/step - loss: 1.3447 - acc: 0.3359 - val_loss: 1.3421 - val_acc: 0.3435\n",
      "Epoch 5/100\n",
      "30589/30589 [==============================] - 1s 17us/step - loss: 1.3418 - acc: 0.3370 - val_loss: 1.3391 - val_acc: 0.3418\n",
      "Epoch 6/100\n",
      "30589/30589 [==============================] - 1s 19us/step - loss: 1.3391 - acc: 0.3426 - val_loss: 1.3383 - val_acc: 0.3405\n",
      "Epoch 7/100\n",
      "30589/30589 [==============================] - 1s 18us/step - loss: 1.3364 - acc: 0.3443 - val_loss: 1.3386 - val_acc: 0.3428\n",
      "Epoch 8/100\n",
      "30589/30589 [==============================] - 1s 18us/step - loss: 1.3339 - acc: 0.3437 - val_loss: 1.3389 - val_acc: 0.3426\n",
      "Epoch 9/100\n",
      "30589/30589 [==============================] - 1s 19us/step - loss: 1.3307 - acc: 0.3459 - val_loss: 1.3403 - val_acc: 0.3424\n",
      "Epoch 10/100\n",
      "30589/30589 [==============================] - 1s 18us/step - loss: 1.3278 - acc: 0.3479 - val_loss: 1.3415 - val_acc: 0.3415\n",
      "Epoch 11/100\n",
      "30589/30589 [==============================] - 1s 28us/step - loss: 1.3256 - acc: 0.3484 - val_loss: 1.3428 - val_acc: 0.3407\n",
      "Epoch 12/100\n",
      "30589/30589 [==============================] - 1s 18us/step - loss: 1.3227 - acc: 0.3526 - val_loss: 1.3444 - val_acc: 0.3427\n",
      "Epoch 13/100\n",
      "30589/30589 [==============================] - 1s 19us/step - loss: 1.3202 - acc: 0.3552 - val_loss: 1.3455 - val_acc: 0.3431\n",
      "Epoch 14/100\n",
      "30589/30589 [==============================] - 1s 22us/step - loss: 1.3169 - acc: 0.3544 - val_loss: 1.3421 - val_acc: 0.3444\n",
      "Epoch 15/100\n",
      "30589/30589 [==============================] - 1s 20us/step - loss: 1.3145 - acc: 0.3593 - val_loss: 1.3460 - val_acc: 0.3398\n",
      "Epoch 16/100\n",
      "30589/30589 [==============================] - 1s 19us/step - loss: 1.3113 - acc: 0.3612 - val_loss: 1.3481 - val_acc: 0.3440\n",
      "Epoch 17/100\n",
      "30589/30589 [==============================] - 1s 18us/step - loss: 1.3091 - acc: 0.3591 - val_loss: 1.3497 - val_acc: 0.3398\n",
      "Epoch 18/100\n",
      "30589/30589 [==============================] - 1s 18us/step - loss: 1.3064 - acc: 0.3645 - val_loss: 1.3531 - val_acc: 0.3371\n",
      "Epoch 19/100\n",
      "30589/30589 [==============================] - 1s 18us/step - loss: 1.3044 - acc: 0.3657 - val_loss: 1.3531 - val_acc: 0.3430\n",
      "Epoch 20/100\n",
      "30589/30589 [==============================] - 1s 18us/step - loss: 1.3014 - acc: 0.3671 - val_loss: 1.3573 - val_acc: 0.3434\n",
      "Epoch 21/100\n",
      "30589/30589 [==============================] - 1s 20us/step - loss: 1.2989 - acc: 0.3694 - val_loss: 1.3554 - val_acc: 0.3423\n",
      "Epoch 22/100\n",
      "30589/30589 [==============================] - 1s 19us/step - loss: 1.2961 - acc: 0.3724 - val_loss: 1.3611 - val_acc: 0.3393\n",
      "Epoch 23/100\n",
      "30589/30589 [==============================] - 1s 19us/step - loss: 1.2934 - acc: 0.3707 - val_loss: 1.3614 - val_acc: 0.3367\n",
      "Epoch 24/100\n",
      "30589/30589 [==============================] - 1s 18us/step - loss: 1.2912 - acc: 0.3727 - val_loss: 1.3657 - val_acc: 0.3370\n",
      "Epoch 25/100\n",
      "30589/30589 [==============================] - 1s 21us/step - loss: 1.2890 - acc: 0.3744 - val_loss: 1.3678 - val_acc: 0.3428\n",
      "Epoch 26/100\n",
      "30589/30589 [==============================] - 1s 18us/step - loss: 1.2866 - acc: 0.3741 - val_loss: 1.3674 - val_acc: 0.3367\n",
      "Epoch 27/100\n",
      "30589/30589 [==============================] - 1s 19us/step - loss: 1.2853 - acc: 0.3752 - val_loss: 1.3764 - val_acc: 0.3338\n",
      "Epoch 28/100\n",
      "30589/30589 [==============================] - 1s 21us/step - loss: 1.2829 - acc: 0.3761 - val_loss: 1.3721 - val_acc: 0.3363\n",
      "Epoch 29/100\n",
      "30589/30589 [==============================] - 1s 19us/step - loss: 1.2801 - acc: 0.3794 - val_loss: 1.3786 - val_acc: 0.3452\n",
      "Epoch 30/100\n",
      "30589/30589 [==============================] - 1s 17us/step - loss: 1.2788 - acc: 0.3802 - val_loss: 1.3760 - val_acc: 0.3322\n",
      "Epoch 31/100\n",
      "30589/30589 [==============================] - 1s 17us/step - loss: 1.2761 - acc: 0.3805 - val_loss: 1.3850 - val_acc: 0.3401\n",
      "Epoch 32/100\n",
      "30589/30589 [==============================] - 1s 17us/step - loss: 1.2747 - acc: 0.3797 - val_loss: 1.3800 - val_acc: 0.3409\n",
      "Epoch 33/100\n",
      "30589/30589 [==============================] - 1s 17us/step - loss: 1.2730 - acc: 0.3841 - val_loss: 1.3891 - val_acc: 0.3388\n",
      "Epoch 34/100\n",
      "30589/30589 [==============================] - 1s 17us/step - loss: 1.2711 - acc: 0.3829 - val_loss: 1.3899 - val_acc: 0.3388\n",
      "Epoch 35/100\n",
      "30589/30589 [==============================] - 1s 17us/step - loss: 1.2696 - acc: 0.3856 - val_loss: 1.3924 - val_acc: 0.3321\n",
      "Epoch 36/100\n",
      "30589/30589 [==============================] - 1s 17us/step - loss: 1.2675 - acc: 0.3851 - val_loss: 1.4001 - val_acc: 0.3411\n",
      "Epoch 37/100\n",
      "30589/30589 [==============================] - 1s 17us/step - loss: 1.2663 - acc: 0.3859 - val_loss: 1.3959 - val_acc: 0.3389\n",
      "Epoch 38/100\n",
      "30589/30589 [==============================] - 1s 17us/step - loss: 1.2645 - acc: 0.3867 - val_loss: 1.3976 - val_acc: 0.3434\n",
      "Epoch 39/100\n",
      "30589/30589 [==============================] - 1s 17us/step - loss: 1.2626 - acc: 0.3873 - val_loss: 1.4044 - val_acc: 0.3389\n",
      "Epoch 40/100\n",
      "30589/30589 [==============================] - 1s 17us/step - loss: 1.2618 - acc: 0.3838 - val_loss: 1.4050 - val_acc: 0.3414\n",
      "Epoch 41/100\n",
      "30589/30589 [==============================] - 1s 17us/step - loss: 1.2598 - acc: 0.3882 - val_loss: 1.4154 - val_acc: 0.3389\n",
      "Epoch 42/100\n",
      "30589/30589 [==============================] - 1s 20us/step - loss: 1.2584 - acc: 0.3889 - val_loss: 1.4134 - val_acc: 0.3443\n",
      "Epoch 43/100\n",
      "30589/30589 [==============================] - 1s 17us/step - loss: 1.2563 - acc: 0.3923 - val_loss: 1.4176 - val_acc: 0.3383\n",
      "Epoch 44/100\n",
      "30589/30589 [==============================] - 1s 17us/step - loss: 1.2559 - acc: 0.3894 - val_loss: 1.4181 - val_acc: 0.3392\n",
      "Epoch 45/100\n",
      "30589/30589 [==============================] - 1s 17us/step - loss: 1.2545 - acc: 0.3879 - val_loss: 1.4127 - val_acc: 0.3407\n",
      "Epoch 46/100\n",
      "30589/30589 [==============================] - 1s 18us/step - loss: 1.2527 - acc: 0.3925 - val_loss: 1.4213 - val_acc: 0.3410\n",
      "Epoch 47/100\n",
      "30589/30589 [==============================] - 1s 23us/step - loss: 1.2522 - acc: 0.3918 - val_loss: 1.4326 - val_acc: 0.3333\n",
      "Epoch 48/100\n",
      "30589/30589 [==============================] - 1s 19us/step - loss: 1.2514 - acc: 0.3907 - val_loss: 1.4332 - val_acc: 0.3426\n",
      "Epoch 49/100\n",
      "30589/30589 [==============================] - 1s 18us/step - loss: 1.2492 - acc: 0.3939 - val_loss: 1.4316 - val_acc: 0.3432\n",
      "Epoch 50/100\n",
      "30589/30589 [==============================] - 1s 19us/step - loss: 1.2486 - acc: 0.3923 - val_loss: 1.4320 - val_acc: 0.3356\n",
      "Epoch 51/100\n",
      "30589/30589 [==============================] - 1s 21us/step - loss: 1.2469 - acc: 0.3935 - val_loss: 1.4350 - val_acc: 0.3364\n",
      "Epoch 52/100\n",
      "30589/30589 [==============================] - 1s 21us/step - loss: 1.2472 - acc: 0.3915 - val_loss: 1.4380 - val_acc: 0.3358\n",
      "Epoch 53/100\n",
      "30589/30589 [==============================] - 1s 19us/step - loss: 1.2456 - acc: 0.3931 - val_loss: 1.4449 - val_acc: 0.3379\n",
      "Epoch 54/100\n",
      "30589/30589 [==============================] - 1s 18us/step - loss: 1.2442 - acc: 0.3943 - val_loss: 1.4402 - val_acc: 0.3350\n",
      "Epoch 55/100\n",
      "30589/30589 [==============================] - 1s 17us/step - loss: 1.2440 - acc: 0.3946 - val_loss: 1.4530 - val_acc: 0.3406\n",
      "Epoch 56/100\n",
      "30589/30589 [==============================] - 1s 17us/step - loss: 1.2424 - acc: 0.3957 - val_loss: 1.4565 - val_acc: 0.3397\n",
      "Epoch 57/100\n",
      "30589/30589 [==============================] - 1s 18us/step - loss: 1.2424 - acc: 0.3943 - val_loss: 1.4411 - val_acc: 0.3413\n",
      "Epoch 58/100\n",
      "30589/30589 [==============================] - 1s 23us/step - loss: 1.2412 - acc: 0.3967 - val_loss: 1.4655 - val_acc: 0.3390\n",
      "Epoch 59/100\n",
      "30589/30589 [==============================] - 1s 17us/step - loss: 1.2398 - acc: 0.3972 - val_loss: 1.4642 - val_acc: 0.3411\n",
      "Epoch 60/100\n",
      "30589/30589 [==============================] - 1s 18us/step - loss: 1.2397 - acc: 0.3969 - val_loss: 1.4545 - val_acc: 0.3406\n",
      "Epoch 61/100\n",
      "30589/30589 [==============================] - 1s 23us/step - loss: 1.2380 - acc: 0.3969 - val_loss: 1.4704 - val_acc: 0.3363\n",
      "Epoch 62/100\n",
      "30589/30589 [==============================] - 1s 19us/step - loss: 1.2387 - acc: 0.3955 - val_loss: 1.4690 - val_acc: 0.3409\n",
      "Epoch 63/100\n",
      "30589/30589 [==============================] - 1s 20us/step - loss: 1.2374 - acc: 0.3980 - val_loss: 1.4644 - val_acc: 0.3336\n",
      "Epoch 64/100\n",
      "30589/30589 [==============================] - 1s 18us/step - loss: 1.2372 - acc: 0.3973 - val_loss: 1.4641 - val_acc: 0.3362\n",
      "Epoch 65/100\n",
      "30589/30589 [==============================] - 1s 20us/step - loss: 1.2366 - acc: 0.3957 - val_loss: 1.4791 - val_acc: 0.3342\n",
      "Epoch 66/100\n",
      "30589/30589 [==============================] - 1s 23us/step - loss: 1.2353 - acc: 0.3982 - val_loss: 1.4738 - val_acc: 0.3339\n",
      "Epoch 67/100\n",
      "30589/30589 [==============================] - 1s 19us/step - loss: 1.2350 - acc: 0.3979 - val_loss: 1.4686 - val_acc: 0.3355\n",
      "Epoch 68/100\n",
      "30589/30589 [==============================] - 1s 19us/step - loss: 1.2350 - acc: 0.3968 - val_loss: 1.4887 - val_acc: 0.3368\n",
      "Epoch 69/100\n",
      "30589/30589 [==============================] - 1s 19us/step - loss: 1.2343 - acc: 0.3981 - val_loss: 1.4698 - val_acc: 0.3356\n",
      "Epoch 70/100\n",
      "30589/30589 [==============================] - 1s 18us/step - loss: 1.2341 - acc: 0.3995 - val_loss: 1.4783 - val_acc: 0.3347\n",
      "Epoch 71/100\n",
      "30589/30589 [==============================] - 1s 18us/step - loss: 1.2342 - acc: 0.3959 - val_loss: 1.4784 - val_acc: 0.3349\n",
      "Epoch 72/100\n",
      "30589/30589 [==============================] - 1s 18us/step - loss: 1.2330 - acc: 0.4003 - val_loss: 1.4917 - val_acc: 0.3342\n",
      "Epoch 73/100\n",
      "30589/30589 [==============================] - 1s 18us/step - loss: 1.2330 - acc: 0.3965 - val_loss: 1.4864 - val_acc: 0.3401\n",
      "Epoch 74/100\n",
      "30589/30589 [==============================] - 1s 18us/step - loss: 1.2322 - acc: 0.4001 - val_loss: 1.4853 - val_acc: 0.3384\n",
      "Epoch 75/100\n",
      "30589/30589 [==============================] - 1s 18us/step - loss: 1.2320 - acc: 0.3969 - val_loss: 1.4855 - val_acc: 0.3347\n",
      "Epoch 76/100\n",
      "30589/30589 [==============================] - 1s 17us/step - loss: 1.2312 - acc: 0.4013 - val_loss: 1.4796 - val_acc: 0.3380\n",
      "Epoch 77/100\n",
      "30589/30589 [==============================] - 1s 17us/step - loss: 1.2311 - acc: 0.3995 - val_loss: 1.4904 - val_acc: 0.3385\n",
      "Epoch 78/100\n",
      "30589/30589 [==============================] - 1s 18us/step - loss: 1.2304 - acc: 0.3981 - val_loss: 1.5070 - val_acc: 0.3356\n",
      "Epoch 79/100\n",
      "30589/30589 [==============================] - 1s 20us/step - loss: 1.2309 - acc: 0.3987 - val_loss: 1.4902 - val_acc: 0.3452\n",
      "Epoch 80/100\n",
      "30589/30589 [==============================] - 1s 18us/step - loss: 1.2295 - acc: 0.3995 - val_loss: 1.4886 - val_acc: 0.3353\n",
      "Epoch 81/100\n",
      "30589/30589 [==============================] - 1s 20us/step - loss: 1.2294 - acc: 0.4013 - val_loss: 1.5017 - val_acc: 0.3389\n",
      "Epoch 82/100\n",
      "30589/30589 [==============================] - 1s 18us/step - loss: 1.2296 - acc: 0.4016 - val_loss: 1.5087 - val_acc: 0.3384\n",
      "Epoch 83/100\n",
      "30589/30589 [==============================] - 1s 19us/step - loss: 1.2283 - acc: 0.4003 - val_loss: 1.5007 - val_acc: 0.3394\n",
      "Epoch 84/100\n",
      "30589/30589 [==============================] - 1s 20us/step - loss: 1.2290 - acc: 0.3990 - val_loss: 1.5036 - val_acc: 0.3385\n",
      "Epoch 85/100\n",
      "30589/30589 [==============================] - 1s 19us/step - loss: 1.2282 - acc: 0.4007 - val_loss: 1.5069 - val_acc: 0.3390\n",
      "Epoch 86/100\n",
      "30589/30589 [==============================] - 1s 18us/step - loss: 1.2274 - acc: 0.4009 - val_loss: 1.5052 - val_acc: 0.3404\n",
      "Epoch 87/100\n",
      "30589/30589 [==============================] - 1s 20us/step - loss: 1.2290 - acc: 0.3981 - val_loss: 1.5136 - val_acc: 0.3411\n",
      "Epoch 88/100\n",
      "30589/30589 [==============================] - 1s 18us/step - loss: 1.2281 - acc: 0.4029 - val_loss: 1.5063 - val_acc: 0.3392\n",
      "Epoch 89/100\n",
      "30589/30589 [==============================] - 1s 20us/step - loss: 1.2272 - acc: 0.3981 - val_loss: 1.5014 - val_acc: 0.3385A: 0s - loss: 1.2235 - acc: 0.\n",
      "Epoch 90/100\n",
      "30589/30589 [==============================] - 1s 19us/step - loss: 1.2267 - acc: 0.4019 - val_loss: 1.5077 - val_acc: 0.3409\n",
      "Epoch 91/100\n",
      "30589/30589 [==============================] - 1s 18us/step - loss: 1.2269 - acc: 0.4001 - val_loss: 1.4924 - val_acc: 0.3337\n",
      "Epoch 92/100\n",
      "30589/30589 [==============================] - 1s 19us/step - loss: 1.2269 - acc: 0.4000 - val_loss: 1.5134 - val_acc: 0.3387\n",
      "Epoch 93/100\n",
      "30589/30589 [==============================] - 1s 19us/step - loss: 1.2261 - acc: 0.4012 - val_loss: 1.4992 - val_acc: 0.3346\n",
      "Epoch 94/100\n",
      "30589/30589 [==============================] - 1s 18us/step - loss: 1.2268 - acc: 0.4044 - val_loss: 1.5167 - val_acc: 0.3359\n",
      "Epoch 95/100\n",
      "30589/30589 [==============================] - 1s 19us/step - loss: 1.2258 - acc: 0.4001 - val_loss: 1.5166 - val_acc: 0.3321\n",
      "Epoch 96/100\n",
      "30589/30589 [==============================] - 1s 19us/step - loss: 1.2265 - acc: 0.4004 - val_loss: 1.5143 - val_acc: 0.3376\n",
      "Epoch 97/100\n",
      "30589/30589 [==============================] - 1s 18us/step - loss: 1.2263 - acc: 0.4024 - val_loss: 1.5207 - val_acc: 0.3389\n",
      "Epoch 98/100\n",
      "30589/30589 [==============================] - 1s 20us/step - loss: 1.2262 - acc: 0.3997 - val_loss: 1.5167 - val_acc: 0.3338\n",
      "Epoch 99/100\n",
      "30589/30589 [==============================] - 1s 18us/step - loss: 1.2247 - acc: 0.4002 - val_loss: 1.5225 - val_acc: 0.3387\n",
      "Epoch 100/100\n",
      "30589/30589 [==============================] - 1s 18us/step - loss: 1.2254 - acc: 0.4004 - val_loss: 1.5387 - val_acc: 0.3394\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1281c2c50>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=100, batch_size=128, validation_data=(X_test, Y_test), shuffle=True, callbacks=[tensorboard])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7648/7648 [==============================] - 0s 34us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3543388035506883, 0.33734309623430964]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=X_test, y=Y_test, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oh', 'baby', 'how', 'you', 'doing', 'you', 'know', 'im', 'gonna', 'cut']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0].lyrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
