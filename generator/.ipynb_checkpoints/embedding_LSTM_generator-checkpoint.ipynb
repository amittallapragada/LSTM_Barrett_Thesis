{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku \n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tim mcgraw</td>\n",
       "      <td>\\n\\r\\nhe said the way my blue eyes shined\\nput...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>picture to burn</td>\n",
       "      <td>\\n\\r\\nstate the obvious, i didn't get my perfe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Name                                             Lyrics\n",
       "0       tim mcgraw  \\n\\r\\nhe said the way my blue eyes shined\\nput...\n",
       "1  picture to burn  \\n\\r\\nstate the obvious, i didn't get my perfe..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle('../data/taylor_swift.pkl', compression='gzip')\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make 1 string with all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\r\\nhe said the way my blue eyes shined\\nput those georgia stars to shame that night\\ni said, \"that\\'s a'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\n",
    "for song in data.Lyrics:\n",
    "    text = text + song\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare dataset\n",
    "-chunk data into sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "def data_prep(data):\n",
    "    text = data.lower().split('\\n')\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    #convert our list of split words into sequences\n",
    "    sequences = []\n",
    "    for line in text:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram = token_list[:i+1]\n",
    "            sequences.append(n_gram)\n",
    "    #find the longest sequences length so we can pad the others to that len\n",
    "    max_len = max([len(x) for x in sequences])\n",
    "    #we add our paddings to the beginning of each val\n",
    "    sequences = np.array(pad_sequences(sequences, maxlen=max_len, padding='pre'))\n",
    "    print(sequences.shape)\n",
    "    X, y = sequences[:,:-1], sequences[:,-1]\n",
    "    y = ku.to_categorical(y, num_classes=total_words)\n",
    "    return X, y, total_words, max_len\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30729, 25)\n"
     ]
    }
   ],
   "source": [
    "X,y,total_words,max_len = data_prep(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30729, 24)\n",
      "(24,)\n",
      "25\n",
      "2446\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X[1].shape)\n",
    "print(max_len)\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 24, 50)            122300    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 150)               120600    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2446)              369346    \n",
      "=================================================================\n",
      "Total params: 612,246\n",
      "Trainable params: 612,246\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 50, input_length=max_len-1))\n",
    "model.add(LSTM(150))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "30729/30729 [==============================] - 26s 848us/step - loss: 6.03390s - loss: 6.0\n",
      "Epoch 2/10\n",
      "30729/30729 [==============================] - 26s 845us/step - loss: 5.5103\n",
      "Epoch 3/10\n",
      "30729/30729 [==============================] - 26s 850us/step - loss: 4.9854\n",
      "Epoch 4/10\n",
      "30729/30729 [==============================] - 26s 843us/step - loss: 4.5267\n",
      "Epoch 5/10\n",
      "30729/30729 [==============================] - 29s 929us/step - loss: 4.1103\n",
      "Epoch 6/10\n",
      "30729/30729 [==============================] - 29s 955us/step - loss: 3.7604\n",
      "Epoch 7/10\n",
      "30729/30729 [==============================] - 26s 844us/step - loss: 3.4485\n",
      "Epoch 8/10\n",
      "30729/30729 [==============================] - 26s 854us/step - loss: 3.1855\n",
      "Epoch 9/10\n",
      "30729/30729 [==============================] - 26s 855us/step - loss: 2.9504\n",
      "Epoch 10/10\n",
      "30729/30729 [==============================] - 26s 860us/step - loss: 2.7465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb2d7440f0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, num_of_words_to_gen, max_len):\n",
    "    for j in range(num_of_words_to_gen):\n",
    "        #format token as a model input\n",
    "        seed_token = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        seed_token = pad_sequences([seed_token], maxlen=max_len-1, padding='pre')\n",
    "        #predict\n",
    "        predicted = model.predict_classes(seed_token, verbose=0)\n",
    "        print(predicted)\n",
    "        pred = model.predict(seed_token)\n",
    "        print('model pred: ', pred)\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4]\n",
      "model pred:  [[1.7799412e-09 9.6874461e-03 3.0316454e-03 ... 2.4212113e-13\n",
      "  1.5816868e-14 4.6232840e-09]]\n",
      "[109]\n",
      "model pred:  [[3.06171899e-07 3.34405974e-02 3.51045877e-02 ... 1.24907338e-08\n",
      "  1.72145349e-07 1.19167045e-02]]\n",
      "[21]\n",
      "model pred:  [[7.7609319e-10 6.4764470e-02 1.2180117e-02 ... 7.4218999e-12\n",
      "  5.5445388e-11 1.7969816e-07]]\n",
      "[8]\n",
      "model pred:  [[3.8437267e-09 9.2474863e-02 2.7643953e-02 ... 1.0018329e-07\n",
      "  6.3868676e-08 6.0716772e-04]]\n",
      "[97]\n",
      "model pred:  [[1.8030312e-07 6.2273843e-06 1.2547576e-05 ... 3.6097994e-05\n",
      "  7.3631475e-04 8.0861728e-06]]\n",
      "[893]\n",
      "model pred:  [[1.06896216e-07 1.80245081e-07 4.36654273e-06 ... 1.12209541e-07\n",
      "  1.53980466e-07 8.68665637e-08]]\n",
      "[14]\n",
      "model pred:  [[7.1827877e-10 5.6894529e-03 5.2324412e-03 ... 1.3776544e-12\n",
      "  1.6224958e-13 2.6695899e-09]]\n",
      "[1791]\n",
      "model pred:  [[3.6279127e-07 4.0804148e-03 5.3533819e-04 ... 2.4894882e-07\n",
      "  1.0122638e-07 1.5113659e-03]]\n",
      "[4]\n",
      "model pred:  [[6.6890244e-08 3.1477783e-03 1.8150777e-03 ... 6.3096078e-10\n",
      "  1.7007104e-10 4.9738060e-06]]\n",
      "[100]\n",
      "model pred:  [[4.8790480e-07 3.0713709e-02 8.0870790e-03 ... 4.7775675e-06\n",
      "  9.2473210e-06 4.5911390e-03]]\n",
      "[106]\n",
      "model pred:  [[3.7659483e-08 1.9782176e-03 1.2791588e-04 ... 2.1422282e-09\n",
      "  4.1988257e-09 1.8454161e-06]]\n",
      "[4]\n",
      "model pred:  [[2.9017284e-09 1.9768304e-03 1.0947972e-03 ... 6.6436327e-12\n",
      "  6.4987503e-12 3.2036837e-08]]\n",
      "[100]\n",
      "model pred:  [[6.4483862e-07 2.4257040e-02 1.4778479e-02 ... 5.8123356e-07\n",
      "  1.3636817e-06 1.2121230e-03]]\n",
      "[499]\n",
      "model pred:  [[6.6555437e-08 2.0673997e-03 9.1913331e-05 ... 6.6229544e-10\n",
      "  3.5814964e-09 1.7041515e-06]]\n",
      "[17]\n",
      "model pred:  [[2.7461717e-11 1.4047387e-04 2.6737354e-05 ... 1.0374458e-15\n",
      "  3.6349021e-15 9.8827578e-12]]\n",
      "[5]\n",
      "model pred:  [[1.7916852e-09 9.4457436e-03 5.4089539e-04 ... 1.4384621e-10\n",
      "  5.1772315e-12 4.5008142e-07]]\n",
      "[46]\n",
      "model pred:  [[1.4877531e-07 1.5427752e-03 6.4236566e-04 ... 2.9862846e-09\n",
      "  5.3769988e-09 2.2043068e-06]]\n",
      "[7]\n",
      "model pred:  [[2.6082685e-09 6.4586900e-02 6.0338405e-04 ... 6.9807271e-09\n",
      "  1.9705750e-09 5.9417898e-06]]\n",
      "[13]\n",
      "model pred:  [[4.5827857e-08 3.6411206e-03 1.4717546e-03 ... 4.6818272e-06\n",
      "  5.0327998e-07 4.9314105e-05]]\n",
      "[2002]\n",
      "model pred:  [[1.4267492e-05 9.0087653e-09 1.0815550e-08 ... 1.8257948e-03\n",
      "  6.3806530e-03 2.0745520e-05]]\n",
      "[944]\n",
      "model pred:  [[6.6680521e-05 1.9499118e-06 1.8326690e-06 ... 4.4960238e-04\n",
      "  6.6652521e-04 5.1780831e-04]]\n",
      "[944]\n",
      "model pred:  [[7.6264718e-05 2.8060020e-07 4.3499230e-07 ... 1.4461761e-03\n",
      "  4.8778336e-03 2.3364829e-04]]\n",
      "[1255]\n",
      "model pred:  [[2.6444344e-05 1.9219872e-08 2.8827333e-08 ... 7.7881402e-04\n",
      "  2.5315431e-03 1.3379275e-04]]\n",
      "[1]\n",
      "model pred:  [[1.0669637e-07 5.4988787e-02 5.9273676e-04 ... 4.6143406e-10\n",
      "  6.0750804e-10 3.8899175e-06]]\n",
      "[64]\n",
      "model pred:  [[1.71965524e-07 1.97783904e-03 5.66022354e-05 ... 1.15588016e-07\n",
      "  9.09151954e-09 4.35477523e-06]]\n",
      "[2188]\n",
      "model pred:  [[5.6111556e-05 1.8779481e-07 3.4584920e-07 ... 1.3533193e-03\n",
      "  1.9289341e-03 1.4494952e-04]]\n",
      "[944]\n",
      "model pred:  [[3.9264298e-05 8.3347672e-08 7.4844799e-08 ... 3.8480278e-04\n",
      "  9.6146733e-04 2.9997784e-04]]\n",
      "[1255]\n",
      "model pred:  [[3.5106677e-05 2.7138972e-08 4.4113072e-08 ... 9.0758962e-04\n",
      "  2.5761540e-03 1.4569143e-04]]\n",
      "[1255]\n",
      "model pred:  [[1.4705482e-05 6.6410828e-09 9.3893684e-09 ... 6.4370444e-04\n",
      "  1.6788968e-03 9.4969771e-05]]\n",
      "[93]\n",
      "model pred:  [[3.14334478e-08 1.40370615e-02 1.22420373e-04 ... 3.39122841e-10\n",
      "  3.90863675e-10 1.71105611e-07]]\n",
      "[93]\n",
      "model pred:  [[5.4941605e-08 1.0031868e-02 6.0239126e-04 ... 2.6204836e-10\n",
      "  7.9329948e-10 3.3636238e-07]]\n",
      "[93]\n",
      "model pred:  [[6.4808440e-08 5.9722592e-03 9.3417574e-04 ... 4.8361405e-11\n",
      "  1.4863695e-10 7.4306826e-07]]\n",
      "[93]\n",
      "model pred:  [[9.8904209e-08 6.1141360e-03 1.8258491e-03 ... 5.2416408e-11\n",
      "  1.6602200e-10 2.2964875e-06]]\n",
      "[148]\n",
      "model pred:  [[7.5674699e-08 3.7496064e-03 2.1626051e-03 ... 7.4229164e-11\n",
      "  2.0707240e-10 1.4278060e-06]]\n",
      "[56]\n",
      "model pred:  [[6.2049539e-09 3.9532525e-04 1.3044850e-05 ... 1.5665076e-13\n",
      "  1.7549451e-12 3.2410883e-09]]\n",
      "[6]\n",
      "model pred:  [[1.5932723e-09 3.1335060e-02 9.2229573e-04 ... 5.6110969e-14\n",
      "  3.2103914e-14 2.3869500e-09]]\n",
      "[19]\n",
      "model pred:  [[1.0938039e-07 6.3400315e-03 2.4556232e-04 ... 7.8600621e-08\n",
      "  5.3906679e-09 6.6490206e-06]]\n",
      "[56]\n",
      "model pred:  [[1.04207025e-08 3.48140777e-04 4.28931053e-05 ... 3.96324779e-10\n",
      "  3.82618937e-10 1.11999299e-08]]\n",
      "[7]\n",
      "model pred:  [[3.9057989e-12 1.1219716e-01 1.5768607e-03 ... 6.6276889e-15\n",
      "  9.9464877e-16 2.9959327e-11]]\n",
      "[84]\n",
      "model pred:  [[3.5108798e-09 7.6120044e-03 3.2534003e-03 ... 1.9953374e-08\n",
      "  1.9891275e-09 9.3666113e-06]]\n",
      "[173]\n",
      "model pred:  [[1.4470397e-09 2.3922841e-03 7.4853544e-04 ... 1.0949365e-12\n",
      "  2.2006360e-11 9.7031316e-10]]\n",
      "[944]\n",
      "model pred:  [[1.5422676e-05 8.2947196e-09 1.1949842e-08 ... 5.5230386e-04\n",
      "  1.7103662e-03 1.6357958e-04]]\n",
      "[1255]\n",
      "model pred:  [[2.5127414e-05 3.3942769e-08 4.0047595e-08 ... 8.5671566e-04\n",
      "  2.0960842e-03 1.4444605e-04]]\n",
      "[1255]\n",
      "model pred:  [[2.0249852e-05 1.9136662e-08 2.4066381e-08 ... 8.1683527e-04\n",
      "  1.7952588e-03 1.2911952e-04]]\n",
      "[84]\n",
      "model pred:  [[3.6827172e-09 3.8621414e-02 9.2703762e-04 ... 4.9012595e-12\n",
      "  1.5074959e-11 5.3047795e-08]]\n",
      "[2188]\n",
      "model pred:  [[9.7316279e-06 3.4005541e-09 6.4236207e-09 ... 6.9997692e-04\n",
      "  1.5354685e-03 6.6695415e-05]]\n",
      "[944]\n",
      "model pred:  [[3.1951717e-05 1.4366402e-07 1.0040936e-07 ... 3.4052785e-04\n",
      "  7.3845848e-04 2.7677632e-04]]\n",
      "[1255]\n",
      "model pred:  [[4.7324662e-05 8.6466578e-08 1.2003242e-07 ... 1.0098572e-03\n",
      "  2.6344666e-03 2.0031120e-04]]\n",
      "[1255]\n",
      "model pred:  [[1.9299494e-05 1.3141637e-08 1.8848706e-08 ... 8.2808494e-04\n",
      "  1.9542489e-03 1.0401178e-04]]\n",
      "[1255]\n",
      "model pred:  [[1.5803638e-05 1.1569716e-08 1.4672054e-08 ... 6.9272949e-04\n",
      "  1.6446041e-03 9.6315795e-05]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"those georgia stars and that's like a little kid of cards and we're away and we're lying on me at it all puts her'd her'd bleachers you were newest her'd bleachers bleachers yeah yeah yeah yeah fall down to be down it right will her'd bleachers bleachers right newest her'd bleachers bleachers bleachers\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_text = 'those georgia stars'\n",
    "next_words = 50\n",
    "generate_text(seed_text, next_words, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
